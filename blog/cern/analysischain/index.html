<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>Physics analysis in the LHC era | Xavier Valls Pla</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />

  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="/css/post.min.423aef0742dab2335ffc1d5a5b0232425b83fbb8ff30ab07fa55524587262ef4.css" integrity="sha256-QjrvB0LasjNf/B1aWwIyQluD&#43;7j/MKsH&#43;lVSRYcmLvQ="/>
  
    
    <link type="text/css" rel="stylesheet" href="/css/custom.min.e82b10a78547c055d0865dc60c44d991716134ea551429c086ee814d26150ee1.css" integrity="sha256-6CsQp4VHwFXQhl3GDETZkXFhNOpVFCnAhu6BTSYVDuE="/>
  
  
   
   
    

<script type="application/ld+json">
  
    {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/xvallspl.github.io\/"
      },
      "articleSection" : "blog",
      "name" : "Physics analysis in the LHC era",
      "headline" : "Physics analysis in the LHC era",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2020",
      "datePublished": "2020-06-12 17:21:44 \u002b0200 CEST",
      "dateModified" : "2020-06-12 17:21:44 \u002b0200 CEST",
      "url" : "http:\/\/xvallspl.github.io\/blog\/cern\/analysischain\/",
      "wordCount" : "3356",
      "keywords" : ["Blog"]
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="/blog">blog</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">Physics analysis in the LHC era</h1>
            <time datetime="2020-06-12 17:21:44 &#43;0200 CEST" class="post__date">Jun 12 2020</time> 
          </header>
          <article class="post__content">
              
<p class="warning"> This article was originally within <a href="http://hdl.handle.net/10803/663536"> a longer text</a> dedicated to <a href="../root/">ROOT</a>, hence the references to it.</br></br>
I wrote this a while ago and some numbers, for instance the amount of data processed, might be outdated.
</p>
<p>The LHC is a circular particle accelerator located at the European Organization for Nuclear Research (CERN) and one of the most sophisticated machines ever built by humanity. In order to recreate conditions similar to those just after the Big Bang, the LHC circulates a beam of charged particles over its 27 km of circumference, increasing their momentum until they reach the appropriate energy, and then collides them at one of the 4 particle detectors in its ring: A Large Ion Collider Experiment (ALICE), A Toroidal LHC Apparatus (ATLAS), Compact Muon Solenoid (CMS) and Large Hadron Collider beauty (LHCb).</p>
<p>Collectively, the LHC detectors, or experiments, produce about 40 petabytes of raw data each year from collisions. This massive amount of data must be stored, processed, and analyzed very eﬃciently. This in an extremely complex challenge, and it requires several optimization steps of data selection, identiﬁcation, ﬁltering and reduction until the data is manageable by the data analysis framework of choice, typically ROOT.</p>
<p>This article introduces the end-to-end process of generating and analyzing data, from the production of charged particles out of hydrogen, to the ﬁnal plotting of a physics process generated by a physicist using his personal computer. In Section <a href="#1-the-lhc-chain">1</a> we give an overview of the LHC chain and the diﬀerent experiments. Section <a href="#2-the-analysis-chain">2</a> describes the diﬀerent steps involved and the challenges to overcome in the analysis chain, from data collection until the plot obtained as result of the analysis. Finally, in Section <a href="#3-computing-infrastructure-at-cern">3</a> we expose the computing infrastructure put in place to support the physics analysis and data distribution.</p>
<p>ROOT plays a fundamental role processing LHC data and is involved across
all stages of the analysis chain. Understanding the full process of
generating and analyzing LHC data provides us with a better
comprehension of the source, the magnitude and the complexity of the
data processed by ROOT, and the heterogeneity of the analyses programmed
with it. Moreover, the massive computational infrastructure currently in
place suggests how much we can benefit from maximizing ROOT’s efficiency
in exploiting modern hardware resources (e.g. by leveraging parallelism
at multiple levels), as the amount of data generated will increase
dramatically in the coming years.</p>
<h2 id="1-the-lhc-chain">1. The LHC chain<a class="anchor" href="#1-the-lhc-chain">#</a></h2>
<p>CERN is a scientiﬁc laboratory created in 1954 as an intergovernmental organization in order to spark the study of particle physics, and it is most importantly known by the complex of accelerators it hosts.</p>
<p>A particle accelerator is an extremely complex machine built out of a large number of electromagnetic devices that guide a beam of particles through the path described by its shape. An accelerator aims to increase the energy of charged particles by accelerating them using electromagnetic ﬁelds. Depending on the path of an accelerator, we classify it as a linear or circular accelerator.</p>
<p>At CERN, linear accelerators (LINAC) are mainly used to inject the beam into circular accelerators at the right momentum. The circular accelerators constitute a chain in which the newer accelerators that were built for experimenting at higher energies require the beam to perform previous passes through the smaller, older, circular accelerators to ensure it is is injected into the higher energy accelerators at the appropriate energy. The circular topology of these accelerators allows for as many laps as necessary to increase the momentum of the particles until the desired energy is reached.</p>
<p>A schematic drawing of the CERN accelerator complex, including the LHC chain, is given in Figure 1. There are two beams circulating simultaneously on the accelerator chain, each consisting of a large number of bunches of <em>O</em>(10<sup>11</sup>) protons. For the LHC the chain starts by extracting protons from hydrogen gas and injecting them into the LINAC2. LINAC2 will accelerate the protons to 50 MeV and then inject them into the PS Booster, which will bring them up to 1.4 GeV. Then both beams are injected into the Proton Synchroton (PS) and accelerated to 25 GeV, after which they are transferred to the last link of the chain—and the second biggest accelerator at CERN—the Super Proton Synchroton (SPS), where they will reach the required injection energy for the LHC, 460 GeV. Finally, in the LHC, the beams will be able to reach up to 7 TeV of energy, in preparation for collision in one of the four main detectors placed at strategic points in the LHC cavern.</p>
<figure><a href="https://cds.cern.ch/record/2684277">
    <img src="https://cds.cern.ch/record/2684277/files/CCC-v2019-final-white.png?subformat=icon-1440"
         alt="CERN accelerator complex"/> </a><figcaption>
            <p><em><strong>Figure 1</strong>: The CERN accelerator complex, including the LHC chain. The yellow dots in the LHC represent each one of the four big experiments.</em>
                    <a href="https://cds.cern.ch/record/2684277">CERN</a></p>
        </figcaption>
</figure>

<h3 id="12-main-detectors-in-the-lhc">1.2 Main detectors in the LHC<a class="anchor" href="#12-main-detectors-in-the-lhc">#</a></h3>
<p>Figure 1 shows the four main detectors, also called
experiments, installed in the LHC cavern: CMS, ATLAS, LHCb and ALICE.
Detectors are highly complex machines, built out of a large number of
state of the art materials and components, with the objective of
recording the results of the collision of two particles moving at high
energies. Figure 2 depicts the ATLAS detector and its most
important sections, comparing it with the size of an average human.</p>
<figure><a href="https://mediaarchive.cern.ch/MediaArchive/Photo/Public/2008/0803012/0803012_01/0803012_01-A5-at-72-dpi.jpg">
    <img src="https://mediaarchive.cern.ch/MediaArchive/Photo/Public/2008/0803012/0803012_01/0803012_01-A5-at-72-dpi.jpg"
         alt="The ATLAS detector"/> </a><figcaption>
            <p><em><strong>Figure 2</strong>: The ATLAS detector.</em>
                    <a href="https://cds.cern.ch/record/1095924">ATLAS Experiment</a></p>
        </figcaption>
</figure>

<p>CMS and ATLAS are two general-purpose detectors used to investigate a
wide range of physics. They have complementary characteristics, and they
are often used to validate each other’s experimental results. LHCb is a
detector specialized in b-physics, and its objective is to look for
possible indications for new physics by studying b-decays. ALICE
performs heavy-ions experimentation to detect quark-gluon plasma, a
state of matter thought to have formed just after the Big Bang.</p>
<figure><a href="https://cds.cern.ch/record/2120661">
    <img src="https://cds.cern.ch/record/2120661/files/CMSslice_whiteBackground.png?subformat=icon-1440"
         alt="CMS Slice"/> </a><figcaption>
            <p><em><strong>Figure 3</strong>: Slice of the CMS detector, showing the tracks of different particles.</em>
                    <a href="https://cds.cern.ch/record/2120661">CC by 4.0. CERN, for the benefit of the CMS Collaboration</a></p>
        </figcaption>
</figure>

<p>Particle detectors are situated at the collision points of the LHC beam.
We collide two particles at extremely high energy to decompose them into
smaller more fundamental particles and observe their behaviour, hoping
to improve our understanding of physics. The trajectory of the particles
resulting from the collision is tracked by pixel and strip detectors,
and the particles are detected and identified according to their energy
and their lifetime inside the detector, that is, to which layer of the
detector they penetrate. Figure 3 illustrates this in a
section of the CMS detector. Photons and electrons deposit their energy
in the electromagnetic calorimeter and hadrons in the hadron
calorimeter. Instead, muons go through every layer of the detector until
they are noticed by the muon system.</p>
<h2 id="2-the-analysis-chain">2. The Analysis chain<a class="anchor" href="#2-the-analysis-chain">#</a></h2>
<p>Frequently, experimental physicists express their analysis in terms of
events (collisions) recorded at a certain point in time. These events
are processed in several steps by the analysis chain.</p>
<p>The analysis chain is defined as the necessary actions to observe and
analyze the interesting physics processes resulting from the collisions
in the detector. It covers the work of going from the signals produced
by the collision in the detector to the final analysis visualization and
is divided in three different phases or steps: data acquisition, data
reconstruction and simulation, and physics analysis.</p>
<p>Data acquisition (Section <a href="#21-data-acquisition">2.1</a>) refers to the process of
recording a sustainable number of the physical events happening in the
detector. Data reconstruction (Section <a href="#22-reconstruction-of-physics-objects-from-data">2.2</a>) and
simulation (Section <a href="#23-monte-carlo-generation">2.3</a>) involves reconstructing the particles’
trajectories after the collision and classifying these particles by type
(tagging) from either the raw data acquired from the detector, or from
simulations based on a theoretical model. Finally, physics analysis
(Section <a href="#24-physics-analysis">2.4</a>) focuses on filtering the physics
processes of interest from the reconstructed data and comparing them
against a theoretical model, in search for its experimental validation
or indications of new physics.</p>
<h3 id="21-data-acquisition">2.1 Data acquisition<a class="anchor" href="#21-data-acquisition">#</a></h3>
<p>CMS and ATLAS perform approximately O(10<sup>9</sup>) collisions per second,
generating around 1 megabyte of data each, which results in about 1
petabyte of data per second to record.</p>
<p>Unfortunately, the current state of data acquisition technology is
several orders of magnitude away from being capable of handling such a
massive amount of data. For this reason, we are forced to record data at
a lower rate.</p>
<p>However, collecting data at a slower pace might result in omitting a
large number of interesting events from the physics analysis. To avoid
this situation, detectors implement the triggers, hybrid
software-hardware systems capable of determining the interest of the
events and rejecting those that don’t show signs of interesting physics
processes.</p>
<p>A trigger system is designed with three levels of filters that events
need to pass through before being recorded for offline analysis:</p>
<ul>
<li>
<p>Level-1: hardware-based trigger. Selects events that produce large
energy deposits in the calorimeters or hits in the muon chambers.</p>
</li>
<li>
<p>Level-2: software-based trigger. Selects events based on a
preliminary analysis of the regions of interest identified in
level-1.</p>
</li>
<li>
<p>Level-3: software trigger. Rudimentarily reconstructs the entire
event.</p>
</li>
</ul>
<p>Only the events passing all three filters are stored for further
processing, with the next step being the transformation of this data
into higher level objects used in physics analysis.</p>
<h3 id="22-reconstruction-of-physics-objects-from-data">2.2 Reconstruction of physics objects from data<a class="anchor" href="#22-reconstruction-of-physics-objects-from-data">#</a></h3>
<p>The reconstruction step transforms raw detector information into higher
level physics objects starting from either the events triggered by the
detector’s data acquisition system, or from the events obtained from the
simulation of the behaviour of the detectors. The latter occurs in
programs such as GEANT4<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The output format of this simulation
needs to be exactly like the data generated by the detector so it is
compatible with the same analysis chain.</p>
<p>The reconstruction process requires the performance of (at least) three
operations: <em>tracking</em>, reconstruction of the particle trajectories into
tracks, determining the parameters of the particles at their point of
production and their momentum; <em>vertexing</em>, grouping particles into
vertices, estimating the location of their production point; and
<em>particle identification</em>, classifying particles based on their tracks
(e.g. photons, muons, etc.).</p>
<p>The particle tracking process involves applying pattern recognition,
mapping hits in the detector with specific tracks, and approximating the
track with an equation (fitting), usually a Kalman filter<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. An
extra track refinement step is often added, tuning the pattern
recognition in order to avoid false positives.</p>
<p>Vertexing involves clustering tracks from a specific event that
originated from the same origin point, finding the vertex candidates
through cluster analysis and then performing a fitting step, obtaining
an estimated vertex position as well as the set of particle tracks
associated with that vertex.</p>
<p>The last step, particle identification, aims to associate each
identified track with a type of particle. This is a fundamental step,
for example, to reduce the volume of data stored for offline analysis to
the interesting events, that is, removing background signals which are
not necessary for the data analysis process.</p>
<p>In addition, in this step of the analysis chains we perform other
operations such as calorimeter reconstruction, to measure the energy of
the electromagnetic and hadronic particles, or jet reconstruction, to
combine particles in jets using the tracking and calorimeter
information.</p>
<h3 id="23-monte-carlo-generation">2.3 Monte Carlo generation<a class="anchor" href="#23-monte-carlo-generation">#</a></h3>
<p>The simulation and reconstruction of the detector chain is frequently
the most time-consuming step in the analysis chain.</p>
<p>Often, physicists choose to work with very simplified simulations of the
detector’s observable events . These simulations are produced by event
generators, instances of simulation software used to model and simulate
physics processes described by a theoretical model. They do so with the
aid of Monte Carlo techniques and algorithms, relying on random sampling
to produce events with the same average behaviour and fluctuations as
real data<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>Monte Carlo event generators aim to simulate the experimental
characteristics of the physics processes of interest, and they are a
fundamental tool for HEP, with a wide variety of applications such as
optimizing the design of new detectors for specific physics events,
exploring analysis strategies to be used in real data or interpreting
observed results in terms of the underlying theory.</p>
<p>A large number of Monte Carlo event generators are available for the
simulation and analysis of HEP theory, e.g. Pythia<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, Herwig
<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> or Alpgen<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>
<h3 id="24-physics-analysis">2.4 Physics analysis<a class="anchor" href="#24-physics-analysis">#</a></h3>
<p>The analysis process starts after reconstruction or simulation of the
collisions. Once we obtain the reconstructed data, stored in ROOT format
(Chapter [ch:ROOT]), we apply to it successive campaigns of data
reduction and refinement. These campaigns consist of filtering and
selecting the events relevant for the analysis at hand, resulting in a
reduced dataset. The data reduction process is driven by the physics
processes of interest, e.g. selecting all the events that involve a
Higgs gamma-gamma decay, and aims to produce an amount of data
manageable from the local computing infrastructures of the experiment or
the physicist.</p>
<p>This reduced data is then processed by the analysis software to produce
data frames (tables) and histogram visualizations, to which statistical
inference is applied. This is done by applying sophisticated algorithms,
such as those used for the classification of the signal versus the
background data of an analysis, for regression analysis, or for the
fitting of probabilistic distributions (Chapter [ch:Fitting]) for the
estimation of physical quantities and observables, for instance
estimating the Higgs mass.</p>
<p>Figure 4 displays two visualizations generated from the
analysis of measured Higgs boson properties using the diphoton
<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> and four-lepton<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> decays channels, based on
data collected by the CMS detector. These figures, produced by ROOT,
showcase two of the most common procedures to be applied to
reconstructed data for analysis. In Figure 4a, we aim to fit
the data to a theoretical model. Through fitting, we estimate physical
quantities values (parameters of the fit) to obtain the parameter values
that describe best the distribution of the reconstructed data. Figure
4b plots the reconstructed data (points with error bars)
against several stacked histograms obtained from simulated data based on
theoretical models, representing expected signal and background
distributions.</p>
<figure><a href="http://cds.cern.ch/record/2312121?ln=en">
    <img src="http://cms-results.web.cern.ch/cms-results/public-results/publications/HIG-16-040/CMS-HIG-16-040_Figure_012-c.png"
         alt="CMS Slice"/> </a><figcaption>
            <p><em><strong>(a)</strong>:  Data and signal-plus-background model ﬁts for the vector-boson fusion Higgs production and top-Higgs production categories in the diphoton decay channel. The one (green) and two (yellow) standard deviation bands include the uncertainties in the background component of the ﬁt. The lower panel shows the residuals after the background subtraction.</em>
                    <a href="http://cds.cern.ch/record/2312121?ln=en">The CMS collaboration</a></p>
        </figcaption>
</figure>

<figure><a href="http://cds.cern.ch/record/2272260">
    <img src="http://cds.cern.ch/record/2272260/files/Figure_003-b.png"
         alt="CMS Slice"/> </a><figcaption>
            <p><em><strong>(b)</strong>: Distribution of the reconstructed four-lepton invariant mass m_4ℓ in the low-mass range. Points with error bars represent the data and stacked histograms represent expected signal and background distributions.</em>
                    <a href="http://cds.cern.ch/record/2272260">The CMS collaboration</a></p>
        </figcaption>
</figure>

<p>
<em><strong>Figure 4</strong>: Mass spectra obtained in the CMS Higgs search using different
decay channels, with a significance >5σ. Data collected in 2017
at a center-of-mass energy of 13 TeV and an integrated luminosity of
35.9 fb<sup>-1</sup>.</em> 
</p>
<p>In addition to traditional statistical inference methods, recently there
has been a rise in popularity of multivariate analysis methods based in
machine learning. In the scope of HEP, we benefit from the
(increasingly) intensive use of techniques such as or . These methods
are mainly used for signal-background classification or regression
analysis, e.g. for estimating the particle energy from the calorimeter
data. These methods have proven very effective, dramatically reducing
the time necessary for these processes and improving accuracy.</p>
<p>HEP analyses are typically performed by ROOT, the official LHC data
analysis toolkit. ROOT provides most of the numerical and statistical
methods necessary for the analysis of HEP data. In addition, it offers
several attractive features for HEP analysis, such as a C++ interactive
interpreter for prototyping and interactive exploration of the analysis,
implicit parallelization, new analysis-centric programming paradigms
such as TDataFrame, and, most importantly, a highly efficient subsystem.
ROOT I/O provides an efficient columnar data format, data compression,
serialization of C++ objects, and data structures optimized for dealing
with the extreme necessities of today’s HEP experiments.</p>
<h2 id="3-computing-infrastructure-at-cern">3. Computing infrastructure at CERN<a class="anchor" href="#3-computing-infrastructure-at-cern">#</a></h2>
<p>The LHC experiments generate new data at increasing rates, currently
around 40 petabytes annually. This makes it essential to adopt a
data-centric computing model, where the data, our most important asset,
is at the core of the choices made in the deployment, distribution,
storage and processing of the data, the design of computer centers or
cluster infrastructures, etc. For instance, the storage, networking and
processing resources that would be necessary to perform local analysis
on such large remote data sets would exceed anything offered by the
commercial solutions currently available. Thus, we need to think of
potentially ad-hoc solutions, such as performing the analysis remotely,
as close as possible to the data, to reduce the load on the network and
facilitate low-latency, high-rate access to the data, or creating a
hierarchical federation of interconnected computing centers to improve
data distribution, locality and processing time.</p>
<p>For instance, processing this enormous amount of data requires, at
least, 500000 typical PC processor cores. A computing center as big as
CERN’s still cannot provide enough resources to store and process such a
large amount of data, both in terms of processing and electrical power,
and is estimated to be able to cover only around 20-30% of the storage
and CPU power needed.</p>
<p>This task of unprecedented magnitude and complexity sparked the creation
of a global computing infrastructure, the WLCG<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. The WLCG is an
international collaboration that involves several national and
international grid infrastructures and many institutions, communities
and projects from a wide variety of fields, such as HEP, astrophysics,
earth sciences or biological and medical research.</p>
<p>The WLCG is a multi-tiered, geographically distributed, federation of
computing centers of various sizes orchestrated from central points.
Depending on which tier a computer center is part of, it will provide
different services. In the scope of HEP and, specifically, the LHC data
analysis chain:</p>
<ul>
<li>
<p>Tier-0: A single computing center tier, built at CERN, providing 20%
of the computing power of the WLCG. Stores all the raw data obtained
from the detectors’ data acquisition system, runs the first pass of
reconstruction and orchestrates the distribution of the raw data to
the following tiers.</p>
</li>
<li>
<p>Tier-1: Thirteen large computing centers used for simulation and
reconstruction of the data received from the Tier-0. They keep a
secondary copy of the raw data and distribute the reconstructed data
to the Tier-2 centers.</p>
</li>
<li>
<p>Tier-2: Around 160 smaller computing centers, typically universities
or research centers, with adequate computing power for users to
perform analysis processes, calibration measurements, and the
generation of Monte Carlo simulations. They provide redundancy for
the reconstructed data and also store the results of users’ data
analyses.</p>
</li>
<li>
<p>Tier-3: Local computing clusters managed by local analysis groups or
even individual computers. Although we often refer to them as
Tier-3, the WLCG does not provide any specification or formal
engagement with them.</p>
</li>
</ul>
<p>Although the roles of each tier still hold, recent years have seen an
improvement in and reduction of the cost of network links, and now it is
possible, for instance, to transfer data between centers of the same
tier or to build a set of fast network hubs connecting many Tier-2 to
many Tier-1 and Tier-0.</p>
<!-- 
<p>
    <div style="position: relative; padding-bottom: 56.25%;overflow: hidden;">
        <iframe src="https://www.youtube.com/embed/wZcaX11rYRE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="WLCG Animation"></iframe>
    </div>
    <em><strong>Figure 1</strong>: Animation showing the WLCG tiers location (2014). Note that the Tier-0 center Wigner is now permanently closed.</em> <a href="https://videos.cern.ch/record/1748359">CERN</a>
</p> -->
<p>The WLCG is a fundamental infrastructure for the HEP community and has
been a core concept for the processing of the analysis chain during the
last decade. Proof of that is that the computing model of the LHC
experiments is still especially designed for the exploitation of grid
resources.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Agostinelli, S., Allison, J., Amako, K., Apostolakis, J., Araujo, H., Arce, P., &hellip; &amp; Behner, F. (2003). GEANT4&ndash;a simulation toolkit. <em>Nuclear Instruments and Methods in Physics Research. Section A, Accelerators, Spectrometers, Detectors and Associated Equipment, 506</em>(3), 250-303. <a href="https://inspirehep.net/literature/593382">link</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Kalman ﬁlter <a href="https://en.wikipedia.org/wiki/Kalman_filter">https://en.wikipedia.org/wiki/Kalman_filter</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Siegert, F. (2010). <em>Monte-Carlo event generation for the LHC</em> (No. CERN-THESIS-2010-302). <a href="https://inspirehep.net/literature/1296465">link</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>Pythia: <a href="http://home.thep.lu.se/~torbjorn/pythia81html/Welcome.html">http://home.thep.lu.se/~torbjorn/pythia81html/Welcome.html</a> <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Herwig: <a href="https://herwig.hepforge.org">https://herwig.hepforge.org</a> <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>AlpGen: <a href="http://mlm.web.cern.ch/mlm/alpgen/">http://mlm.web.cern.ch/mlm/alpgen/</a> <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>Sirunyan, A. M., Tumasyan, A., Adam, W., Ambrogi, F., Asilar, E., Bergauer, T., &hellip; &amp; Del Valle, A. E. (2018). Measurements of Higgs boson properties in the diphoton decay channel in proton-proton collisions at $$\sqrt {s}= 13$$ TeV. <em>Journal of High Energy Physics</em> 2018(11), 185. <a href="https://inspirehep.net/literature/1666825">link</a> <a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Sirunyan, A. M., Tumasyan, A., Adam, W., Ambrogi, F., Asilar, E., Bergauer, T., &hellip; &amp; Del Valle, A. E. (2018). Measurements of Higgs boson properties in the diphoton decay channel in proton-proton collisions at $$\sqrt {s}= 13$$ TeV. Journal of High Energy Physics, 2018(11), 185. <a href="https://inspirehep.net/literature/1608162">link</a> <a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>Shiers, J. (2007). The worldwide LHC computing grid (worldwide LCG). <em>Computer physics communications</em>, 177(1-2), 219-223. <a href="http://cds.cern.ch/record/1063847">link</a> <a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>


              
          </article>
          

<ul class="tags__list">
    
    <li class="tag__item">
        <a class="tag__link" href="http://xvallspl.github.io/tags/cern/">cern</a>
    </li></ul>

 <div class="pagination">
  
    <a class="pagination__item" href="http://xvallspl.github.io/blog/cern/root/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">ROOT</span>
    </a>
  

  
    <a class="pagination__item" href="http://xvallspl.github.io/blog/cern/rms/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >Automatization of Techlab&#39;s Request Management Process</a>
    </a>
  
</div>

          
          <footer class="post__footer">
            <div class="social-icons">
    
    
    
    <a class="social-icons__icon social-icons__icon--linkedin"
        href="https://www.linkedin.com/in/xaviervallspla"></a>
    
</div>
            <p></p>
          </footer>
          </div>
      </div>
      
      <div class="toc-container">
          
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-the-lhc-chain">1. The LHC chain</a>
          <ul>
            <li><a href="#12-main-detectors-in-the-lhc">1.2 Main detectors in the LHC</a></li>
          </ul>
        </li>
        <li><a href="#2-the-analysis-chain">2. The Analysis chain</a>
          <ul>
            <li><a href="#21-data-acquisition">2.1 Data acquisition</a></li>
            <li><a href="#22-reconstruction-of-physics-objects-from-data">2.2 Reconstruction of physics objects from data</a></li>
            <li><a href="#23-monte-carlo-generation">2.3 Monte Carlo generation</a></li>
            <li><a href="#24-physics-analysis">2.4 Physics analysis</a></li>
          </ul>
        </li>
        <li><a href="#3-computing-infrastructure-at-cern">3. Computing infrastructure at CERN</a></li>
      </ul>
    </li>
  </ul>
</nav>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="/js/index.min.49e4d8a384357d9b445b87371863419937ede9fa77737522ffb633073aebfa44.js" integrity="sha256-SeTYo4Q1fZtEW4c3GGNBmTft6fp3c3Ui/7YzBzrr&#43;kQ=" crossorigin="anonymous"></script>
  
  

  
    <script src="/js/table-of-contents.js"></script>
  


</body>

</html>
